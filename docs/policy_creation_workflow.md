# Policy Creation Workflow from LLM Suggestions

This document explains how to create policies from LLM suggestions generated by the 5-step observability analysis process.

## Overview

The system provides multiple ways to create policies:

1. **From LLM Suggestions** - Convert AI-generated recommendations into enforceable policies
2. **From Templates** - Use predefined compliance framework templates
3. **From Scratch** - Generate policies from plain English descriptions

## Workflow: LLM Suggestions â†’ Policies

### Step 1: Generate LLM Suggestions

First, run the 5-step analysis to generate suggestions:

```bash
# Via API
curl -X POST http://localhost:8001/api/llm/5step \
  -H "Content-Type: application/json" \
  -d '{
    "service_name": "user-api",
    "description": "User management API service",
    "sli_quantity": 5,
    "slo_quantity": 3,
    "alert_quantity": 3,
    "suggestion_quantity": 5
  }'

# Via Frontend
# Navigate to Analysis page and run 5-step analysis
```

### Step 2: Browse Available Suggestions

#### Via Frontend
1. Go to `/policies` page
2. Click "LLM Suggestions" tab
3. Filter by service name or category
4. Browse suggestions with metadata:
   - Category (risk, coverage, optimization, best_practice)
   - Priority (high, medium, low)
   - Effort and Impact levels
   - AI confidence score

#### Via CLI
```bash
# List all suggestions
python cli/policy.py list-suggestions

# Filter by service
python cli/policy.py list-suggestions --service-name "user-api"

# Filter by category
python cli/policy.py list-suggestions --category "risk"

# Limit results
python cli/policy.py list-suggestions --limit 10
```

#### Via API
```bash
# Get all suggestions
curl http://localhost:8001/api/policies/suggestions

# Filter by service
curl "http://localhost:8001/api/policies/suggestions?service_name=user-api"

# Filter by category
curl "http://localhost:8001/api/policies/suggestions?category=risk"
```

### Step 3: Create Policy from Suggestion

#### Via Frontend
1. Select a suggestion from the list
2. Choose policy type (Rego, YAML, JSON)
3. Select LLM model and temperature
4. Click "Create Policy from Suggestion"

#### Via CLI
```bash
python cli/policy.py create-from-suggestion \
  <llm_output_id> \
  <suggestion_index> \
  --output "output/error_budget_policy.rego" \
  --policy-type "rego" \
  --model "llama2" \
  --temperature 0.3
```

#### Via API
```bash
curl -X POST http://localhost:8001/api/policies/create-from-suggestion \
  -H "Content-Type: application/json" \
  -d '{
    "llm_output_id": "abc123",
    "suggestion_index": 0,
    "policy_type": "rego",
    "model": "llama2",
    "temperature": 0.3
  }'
```

## Example Workflow

### 1. Sample LLM Suggestion
```json
{
  "category": "risk",
  "metric": "error_rate_total",
  "recommendation": "Implement error budget burn rate alerting when error rate exceeds 0.3% for 5 minutes to provide early warning before SLO violation",
  "priority": "high",
  "effort": "low",
  "impact": "high"
}
```

### 2. Generated Rego Policy
```rego
package error_budget_policy

import future.keywords.if
import future.keywords.in

# Error budget burn rate policy
# Generated from LLM suggestion: Implement error budget burn rate alerting when error rate exceeds 0.3% for 5 minutes

default allow = false

# Allow if error rate is within acceptable threshold
allow if {
    input.error_rate <= 0.003  # 0.3%
    input.time_window == "5m"
}

# Deny if error rate exceeds threshold
deny[msg] if {
    input.error_rate > 0.003
    input.time_window == "5m"
    msg := sprintf("Error rate %.3f%% exceeds threshold of 0.3%% for 5-minute window", [input.error_rate * 100])
}

# Alert condition
alert[msg] if {
    input.error_rate > 0.003
    input.time_window == "5m"
    msg := sprintf("ERROR_BUDGET_BURN: Error rate %.3f%% exceeds 0.3%% threshold", [input.error_rate * 100])
}
```

### 3. Generated YAML Policy
```yaml
# Error budget burn rate policy
# Generated from LLM suggestion: Implement error budget burn rate alerting when error rate exceeds 0.3% for 5 minutes

apiVersion: v1
kind: Policy
metadata:
  name: error-budget-burn-rate
  description: "Error budget burn rate alerting policy"
  category: "risk"
  priority: "high"
spec:
  rules:
    - name: "error-rate-threshold"
      condition: "error_rate > 0.003"
      timeWindow: "5m"
      action: "alert"
      severity: "warning"
      message: "Error rate exceeds 0.3% threshold for 5-minute window"
  
  alerts:
    - name: "error-budget-burn"
      condition: "error_rate > 0.003"
      duration: "5m"
      severity: "warning"
      message: "ERROR_BUDGET_BURN: Error rate exceeds 0.3% threshold"
```

## Policy Types Supported

### 1. Rego (Open Policy Agent)
- **Use case**: Kubernetes, cloud infrastructure, API gateways
- **Features**: Declarative policy language, powerful query capabilities
- **Example**: Access control, resource validation, compliance rules

### 2. YAML
- **Use case**: Kubernetes, Helm charts, configuration management
- **Features**: Human-readable, structured data
- **Example**: Resource limits, security policies, deployment rules

### 3. JSON
- **Use case**: API policies, webhook rules, custom validators
- **Features**: Machine-readable, widely supported
- **Example**: Data validation, business rules, integration policies

## Suggestion Categories

### Risk
- **Focus**: Security vulnerabilities, reliability issues
- **Examples**: 
  - "Implement error budget burn rate alerting"
  - "Add circuit breaker pattern for service resilience"
  - "Enable encryption for sensitive data"

### Coverage
- **Focus**: Missing monitoring, observability gaps
- **Examples**:
  - "Add SLI for external API response times"
  - "Monitor CPU and memory saturation"
  - "Track dependency health metrics"

### Optimization
- **Focus**: Performance improvements, efficiency gains
- **Examples**:
  - "Implement Redis caching layer to reduce latency"
  - "Optimize database query performance"
  - "Add connection pooling for better resource utilization"

### Best Practice
- **Focus**: Industry standards, proven patterns
- **Examples**:
  - "Implement distributed tracing"
  - "Add health check endpoints"
  - "Use structured logging"

## Integration with Existing Workflows

### 1. Database Storage
All LLM outputs are stored in the `llm_outputs` table with:
- Complete analysis results
- AI confidence scores
- Metadata and timestamps
- Step-by-step data

### 2. Policy Management
Generated policies are:
- Saved to the `output/` directory
- Tracked with metadata linking to source suggestions
- Available for validation and testing
- Compatible with existing policy workflows

### 3. Continuous Improvement
- Track policy effectiveness over time
- Update policies based on new suggestions
- Maintain audit trail of policy changes
- Integrate with drift detection and scorecard metrics

## Best Practices

### 1. Suggestion Selection
- Prioritize high-impact, low-effort suggestions
- Consider AI confidence scores
- Review suggestions in context of existing policies
- Validate technical feasibility

### 2. Policy Generation
- Start with Rego for infrastructure policies
- Use YAML for Kubernetes-native policies
- Consider JSON for API and integration policies
- Test generated policies before deployment

### 3. Policy Management
- Version control all policies
- Document policy sources and rationale
- Regular policy reviews and updates
- Monitor policy effectiveness

## Troubleshooting

### Common Issues

1. **No suggestions found**
   - Ensure 5-step analysis has been run
   - Check database connectivity
   - Verify LLM output format

2. **Policy generation fails**
   - Check LLM model availability
   - Verify suggestion format
   - Review error logs

3. **Generated policy is invalid**
   - Adjust temperature for more conservative generation
   - Use different LLM model
   - Manually review and edit generated policy

### Debug Commands
```bash
# Check database for LLM outputs
python -c "
import asyncio
from backend.api.models_llm import SessionLocal, LLMOutput
from sqlalchemy.future import select

async def check_outputs():
    async with SessionLocal() as db:
        result = await db.execute(select(LLMOutput))
        outputs = result.scalars().all()
        print(f'Found {len(outputs)} LLM outputs')
        for output in outputs[:3]:
            print(f'ID: {output.id}, Task: {output.task}, Confidence: {output.confidence}')

asyncio.run(check_outputs())
"

# List available suggestions
python cli/policy.py list-suggestions --limit 5

# Test policy generation
python cli/policy.py create-from-suggestion <output_id> 0 --output test.rego
```

## Next Steps

1. **Run 5-step analysis** to generate suggestions
2. **Browse suggestions** in the frontend or CLI
3. **Select high-priority suggestions** for policy creation
4. **Generate and test policies** using the workflow
5. **Deploy and monitor** policy effectiveness
6. **Iterate and improve** based on results 