llm:
  provider: openai
  model: gpt-4
  temperature: 0.7
  max_tokens: 2000

metrics:
  provider: prometheus
  url: http://localhost:9090
  timeout: 1800

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

templates_dir: templates
output_dir: output 